{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f893c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validation Experiment\n",
    "\n",
    "This is a much simplified version of previous validation notebooks. You load the data in, then there is a cell for setting the run parameters, then you run everything in a loop.\n",
    "\n",
    "Feature creation can now come in two forms: time-averaged or PCA. Both methods are used to reduce the dimensionality of the original heatmap so that features can be generated from a reduced dataset.\n",
    "\n",
    "The feature generation function now takes kwargs so that you can specify how the PCA is done (i.e. how many components are kept) or how the features are calculated (i.e. splitting lobe calculations by hemisphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e14833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1b6435",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import collections\n",
    "from natsort import natsorted\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import sys\n",
    "from numpy import interp\n",
    "from pprint import pprint\n",
    "\n",
    "from numpy.testing import assert_array_equal\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelBinarizer, LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    brier_score_loss,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    jaccard_score,\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    precision_score,\n",
    "    plot_precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score,\n",
    "    make_scorer,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedGroupKFold,\n",
    "    cross_validate,\n",
    "    StratifiedShuffleSplit,\n",
    "    LeaveOneGroupOut,\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import mne\n",
    "from mne.time_frequency import read_tfrs\n",
    "\n",
    "mne.set_log_level(\"ERROR\")\n",
    "from mne_bids import BIDSPath, get_entities_from_fname, get_entity_vals, read_raw_bids\n",
    "\n",
    "from eztrack.io import read_derivative_npy\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "# sys.path.append(\"..\\episcalp\")\n",
    "from episcalp.features import spike_feature_vector, heatmap_features, heatmap_features2\n",
    "from episcalp.io.read import (\n",
    "    load_persyst_spikes,\n",
    "    load_reject_log,\n",
    "    load_derivative_heatmaps,\n",
    "    map_rejectlog_to_deriv,\n",
    ")\n",
    "from episcalp.preprocess.montage import _standard_lobes\n",
    "from episcalp.utils.utils import NumpyEncoder\n",
    "from episcalp.cross_validate import exclude_subjects\n",
    "\n",
    "# if you installed sporf via README\n",
    "# from oblique_forests.sporf import ObliqueForestClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056067c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define possible helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f8190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_exp_condition(subject, root):\n",
    "    part_fname = os.path.join(root, \"participants.tsv\")\n",
    "    df = pd.read_csv(part_fname, sep=\"\\t\")\n",
    "\n",
    "    if not subject.startswith(\"sub-\"):\n",
    "        subject = f\"sub-{subject}\"\n",
    "\n",
    "    return df[df[\"participant_id\"] == subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f192ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_experimental_cond_to_y(experimental_condition_list):\n",
    "    \"\"\"Encoder for y labels.\"\"\"\n",
    "    # Group name keys, assigned y-label values\n",
    "    experimental_condition_map = {\n",
    "        \"non-epilepsy-normal-eeg\": 0,\n",
    "        \"epilepsy-normal-eeg\": 1,\n",
    "        \"epilepsy-abnormal-eeg\": 2,\n",
    "    }\n",
    "    return [experimental_condition_map[cond] for cond in experimental_condition_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e287f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_epochs_tfr(data):\n",
    "    \"\"\"Turn TFR data into a 2D array.\"\"\"\n",
    "    assert data.ndim == 4\n",
    "\n",
    "    # take the average over frequencies\n",
    "    data = np.mean(data, axis=2)\n",
    "\n",
    "    # move the epoch (\"window\") axis to last\n",
    "    data = np.moveaxis(data, 0, -2)\n",
    "\n",
    "    # compress the time axis\n",
    "    data = np.mean(data, axis=-1)\n",
    "\n",
    "    # convert to dB\n",
    "    data = 20 * np.log10(data)\n",
    "\n",
    "    data = np.reshape(data, (data.shape[0], -1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf16e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(deriv_dataset):\n",
    "    dataset = deriv_dataset[0]\n",
    "    for deriv in deriv_dataset:\n",
    "        for key in deriv.keys():\n",
    "            if key not in dataset.keys():\n",
    "                raise RuntimeError(\n",
    "                    f\"All keys in {dataset.keys()} must match every other derived dataset. \"\n",
    "                    f\"{key}, {deriv.keys()}.\"\n",
    "                )\n",
    "\n",
    "    # convert to a dictionary of lists\n",
    "    derived_dataset = {key: [] for key in dataset.keys()}\n",
    "    for deriv in deriv_dataset:\n",
    "        for key in derived_dataset.keys():\n",
    "            derived_dataset[key].extend(deriv[key])\n",
    "    return derived_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3438f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b16792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"kristin\"\n",
    "if user == \"patrick\":\n",
    "    jhroot = Path(\"D:/OneDriveParent/OneDrive - Johns Hopkins/Shared Documents/bids\")\n",
    "    jeffroot = Path(\"D:/OneDriveParent/Johns Hopkins/Jefferson_Scalp - Documents/root\")\n",
    "\n",
    "    # not ready yet\n",
    "    upmcroot = Path(\"/Users/adam2392/Johns Hopkins/UPMC_Scalp - Documents/\")\n",
    "    deriv_dir = Path(\n",
    "        \"D:/OneDriveParent/OneDrive - Johns Hopkins/Shared Documents/derivatives\"\n",
    "    )\n",
    "elif user == \"adam\":\n",
    "    jhroot = Path(\"/Users/adam2392/Johns Hopkins/Scalp EEG JHH - Documents/bids/\")\n",
    "    jeffroot = Path(\"/Users/adam2392/Johns Hopkins/Jefferson_Scalp - Documents/root/\")\n",
    "\n",
    "    # not ready yet\n",
    "    upmcroot = Path(\"/Users/adam2392/Johns Hopkins/UPMC_Scalp - Documents/\")\n",
    "    deriv_dir = Path(\n",
    "        \"/Users/adam2392/Johns Hopkins/Scalp EEG JHH - Documents/derivatives\"\n",
    "    )\n",
    "elif user == \"kristin\":\n",
    "#     jhroot = Path(\"/Users/Kristin/OneDrive - Johns Hopkins/Documents - Scalp EEG JHH/bids/\")\n",
    "    jhroot = Path(\"D:/kgunnar1/OneDrive - Johns Hopkins/Documents - Scalp EEG JHH/bids\")\n",
    "#     jeffroot = Path(\"/Users/Kristin/OneDrive - Johns Hopkins/Documents - Jefferson_Scalp/root/\")\n",
    "    jeffroot = Path(\"D:/kgunnar1/OneDrive - Johns Hopkins/Documents - Jefferson_Scalp/root\")  \n",
    "\n",
    "    # not ready yet\n",
    "#     upmcroot = Path(\"/Users/Kristin/OneDrive - Johns Hopkins/Documents - UPMC_Scalp\")\n",
    "    upmcroot = Path(\"D:/kgunnar1/OneDrive - Johns Hopkins/Documents - UPMC_Scalp\")\n",
    "#     deriv_dir = Path(\"/Users/Kristin/OneDrive - Johns Hopkins/Documents - Scalp EEG JHH/derivatives\")\n",
    "    deriv_dir = Path(\"D:/kgunnar1/OneDrive - Johns Hopkins/Documents - Scalp EEG JHH/derivatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593f89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_roots = [jhroot, jeffroot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c147823",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"monopolar\"\n",
    "radius = \"1.25\"\n",
    "\n",
    "# define different derivative chains\n",
    "ss_deriv_chain = Path(\"sourcesink\") / \"win-500\" / \"step-250\" / reference\n",
    "frag_deriv_chain = (\n",
    "    Path(\"fragility\") / f\"radius{radius}\" / \"win-500\" / \"step-250\" / reference\n",
    ")\n",
    "\n",
    "delta_tfr_deriv_chain = Path(\"tfr\") / \"delta\"\n",
    "theta_tfr_deriv_chain = Path(\"tfr\") / \"theta\"\n",
    "alpha_tfr_deriv_chain = Path(\"tfr\") / \"alpha\"\n",
    "beta_tfr_deriv_chain = Path(\"tfr\") / \"beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70bb7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_reprs = [\n",
    "    ss_deriv_chain,\n",
    "    frag_deriv_chain,\n",
    "    delta_tfr_deriv_chain,\n",
    "    theta_tfr_deriv_chain,\n",
    "    alpha_tfr_deriv_chain,\n",
    "    beta_tfr_deriv_chain,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded2ba9",
   "metadata": {},
   "source": [
    "# Load The Data (Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fca6011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fragility for D:\\kgunnar1\\OneDrive - Johns Hopkins\\Documents - Scalp EEG JHH\\bids\n",
      "Loading fragility for D:\\kgunnar1\\OneDrive - Johns Hopkins\\Documents - Jefferson_Scalp\\root\n",
      "112\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "# load fragility data\n",
    "datasets = []\n",
    "kwargs = {\"preload\": \"True\"}\n",
    "for root in bids_roots:\n",
    "    print(f\"Loading fragility for {root}\")\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / frag_deriv_chain,\n",
    "        search_str=\"*desc-perturbmatrix*.npy\",\n",
    "        read_func=read_derivative_npy,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "        source_check=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "fragility_dataset = combine_datasets(datasets)\n",
    "print(len(dataset[\"subject\"]))\n",
    "print(len(fragility_dataset[\"subject\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51a16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three SS module data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / ss_deriv_chain,\n",
    "        search_str=\"*desc-ssindmatrix*.npy\",\n",
    "        read_func=read_derivative_npy,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "        source_check=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "ss_dataset = combine_datasets(datasets)\n",
    "\n",
    "\n",
    "# load all three SS module data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / ss_deriv_chain,\n",
    "        search_str=\"*desc-sourceinflmatrix*.npy\",\n",
    "        read_func=read_derivative_npy,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "        source_check=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "sourceinfl_dataset = combine_datasets(datasets)\n",
    "\n",
    "# load all three SS module data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / ss_deriv_chain,\n",
    "        search_str=\"*desc-sinkconn*.npy\",\n",
    "        read_func=read_derivative_npy,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "        source_check=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "sinkconn_dataset = combine_datasets(datasets)\n",
    "\n",
    "# load all three SS module data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / ss_deriv_chain,\n",
    "        search_str=\"*desc-sinkind*.npy\",\n",
    "        read_func=read_derivative_npy,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "        source_check=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "sinkind_dataset = combine_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8cde4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "233\n",
      "233\n",
      "233\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"subject\"]))\n",
    "print(len(ss_dataset[\"subject\"]))\n",
    "print(len(sinkind_dataset[\"subject\"]))\n",
    "print(len(sinkconn_dataset[\"subject\"]))\n",
    "print(len(sourceinfl_dataset[\"subject\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c544fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_tfrs_lamb = lambda x: read_tfrs(x)[0]\n",
    "\n",
    "# load TFR data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    freq_band = \"delta\"\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / delta_tfr_deriv_chain,\n",
    "        search_str=f\"*desc-{freq_band}*.h5\",\n",
    "        read_func=read_tfrs_lamb,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "delta_dataset = combine_datasets(datasets)\n",
    "\n",
    "# load TFR data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    freq_band = \"theta\"\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / theta_tfr_deriv_chain,\n",
    "        search_str=f\"*desc-{freq_band}*.h5\",\n",
    "        read_func=read_tfrs_lamb,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "theta_dataset = combine_datasets(datasets)\n",
    "\n",
    "# load TFR data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    freq_band = \"alpha\"\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / alpha_tfr_deriv_chain,\n",
    "        search_str=f\"*desc-{freq_band}*.h5\",\n",
    "        read_func=read_tfrs_lamb,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "alpha_dataset = combine_datasets(datasets)\n",
    "\n",
    "# load TFR data\n",
    "datasets = []\n",
    "for root in bids_roots:\n",
    "    freq_band = \"beta\"\n",
    "    dataset = load_derivative_heatmaps(\n",
    "        root / \"derivatives\" / beta_tfr_deriv_chain,\n",
    "        search_str=f\"*desc-{freq_band}*.h5\",\n",
    "        read_func=read_tfrs_lamb,\n",
    "        subjects=None,\n",
    "        verbose=False,\n",
    "    )\n",
    "    datasets.append(dataset)\n",
    "beta_dataset = combine_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d384506",
   "metadata": {},
   "source": [
    "## Define Sets of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09289189",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_mapping = {\n",
    "    \"fragility\": fragility_dataset,\n",
    "    #     \"ss\": ss_dataset,\n",
    "    \"sourceinfl\": sourceinfl_dataset,\n",
    "    \"sinkind\": sinkind_dataset,\n",
    "    \"sinkconn\": sinkconn_dataset,\n",
    "    \"delta\": delta_dataset,\n",
    "    \"theta\": theta_dataset,\n",
    "    \"alpha\": alpha_dataset,\n",
    "    \"beta\": beta_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87331b35",
   "metadata": {},
   "source": [
    "# Define Run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33c2ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"kristin\"\n",
    "# Pass a list of features for average and pca summary methods\n",
    "feature_type_dict = {\n",
    "    \"average\": [\"lobes\"],\n",
    "    \"pca\": [],\n",
    "    \"variance\": [],\n",
    "    \"singular_values\": []\n",
    "    #     \"singular_values\": [\"first_n\"]\n",
    "}\n",
    "# Pass any kwargs possibly needed\n",
    "# TODO: expand to allow things like only returning the mean per lobe\n",
    "kwargs = {\n",
    "    \"n_keep\": 2,\n",
    "    # \"n_components\": 2,\n",
    "    \"separate_hemispheres\": False,\n",
    "}\n",
    "\n",
    "# List of metrics to include\n",
    "metric_names = [\n",
    "    \"fragility\",\n",
    "    #     \"ss\",\n",
    "    \"sinkind\",\n",
    "    \"sinkconn\",\n",
    "    \"sourceinfl\",\n",
    "    \"delta\",\n",
    "    \"theta\",\n",
    "    \"alpha\",\n",
    "    \"beta\",\n",
    "]\n",
    "\n",
    "winsize = 500\n",
    "stepsize = 250\n",
    "radius = 1.25\n",
    "reference = \"monopolar\"\n",
    "\n",
    "# Binary exclusion criteria - columns from the participants.tsv\n",
    "categorical_exclusion_criteria = {\n",
    "    \"exp_condition\": [\"epilepsy-abnormal-eeg\"],\n",
    "    \"final_diagnosis\": None,\n",
    "    \"epilepsy_type\": [\"generalized\"],\n",
    "    \"epilepsy_hemisphere\": None,\n",
    "    \"epilepsy_lobe\": None,\n",
    "}\n",
    "continuous_exclusion_criteria = {\n",
    "    \"age\": None,\n",
    "    \"num_aeds\": None,\n",
    "}\n",
    "\n",
    "# Cross validation parameters\n",
    "n_splits = 20\n",
    "train_size = 0.7\n",
    "random_state = 12345\n",
    "\n",
    "# Define Classifier parameters\n",
    "clf_name = \"lr\"\n",
    "rf_model_params = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": random_state,\n",
    "}\n",
    "lr_model_params = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": random_state,\n",
    "    \"penalty\": \"l1\",\n",
    "    \"solver\": \"liblinear\",\n",
    "}\n",
    "\n",
    "\n",
    "exp_name = \"heatmap_feats2_lobes_avg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ed506",
   "metadata": {},
   "source": [
    "# Run Experiment in a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f0cb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "y_enc = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7db9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_cv = StratifiedShuffleSplit(\n",
    "    n_splits=n_splits, train_size=train_size, random_state=random_state,\n",
    ")\n",
    "log_cv = LeaveOneGroupOut()\n",
    "# cv = BootstrapSplit(n_splits=100, random_state=random_state)\n",
    "\n",
    "cvs = {\n",
    "    \"stratifiedshuffle\": stratified_cv,\n",
    "    #        \"leaveonesubout\": log_cv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41816423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'balanced_accuracy': make_scorer(balanced_accuracy_score), 'cohen_kappa_score': make_scorer(cohen_kappa_score), 'roc_auc': 'roc_auc', 'f1': 'f1', 'recall': 'recall', 'precision': 'precision', 'jaccard': 'jaccard', 'average_precision': 'average_precision', 'neg_brier_score': 'neg_brier_score'}\n"
     ]
    }
   ],
   "source": [
    "scoring_funcs = {\n",
    "    \"balanced_accuracy\": make_scorer(balanced_accuracy_score),\n",
    "    \"cohen_kappa_score\": make_scorer(cohen_kappa_score),\n",
    "    \"roc_auc\": \"roc_auc\",  # roc_auc_score,\n",
    "    \"f1\": \"f1\",  # f1_score,\n",
    "    \"recall\": \"recall\",  # makerecall_score,\n",
    "    \"precision\": \"precision\",  # precision_score,\n",
    "    \"jaccard\": \"jaccard\",  # jaccard_score,\n",
    "    \"average_precision\": \"average_precision\",  # average_precision_score,\n",
    "    \"neg_brier_score\": \"neg_brier_score\",  # brier_score_loss,\n",
    "}\n",
    "\n",
    "scoring = scoring_funcs\n",
    "print(scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c644349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File D:\\kgunnar1\\OneDrive - Johns Hopkins\\Documents - Scalp EEG JHH\\derivatives\\normaleeg\\lr\\heatmap_feats2_lobes_avg_features.csv exists False\n"
     ]
    }
   ],
   "source": [
    "fname = deriv_dir / \"normaleeg\" / clf_name / f\"{exp_name}_features.csv\"\n",
    "fname.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"File {fname} exists {fname.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "345cf097",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metrics: ('fragility',)\n",
      "Class 0 has  80\n",
      "Class 1 has  53\n",
      "Using metrics: ('sinkind',)\n",
      "Class 0 has  80\n",
      "Class 1 has  53\n",
      "Using metrics: ('sinkconn',)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m summary_method, feature_types \u001b[38;5;129;01min\u001b[39;00m feature_type_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_types:\n\u001b[1;32m---> 19\u001b[0m         _feature_vec \u001b[38;5;241m=\u001b[39m heatmap_features2(\n\u001b[0;32m     20\u001b[0m             data,\n\u001b[0;32m     21\u001b[0m             ch_names\u001b[38;5;241m=\u001b[39mch_names,\n\u001b[0;32m     22\u001b[0m             types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m     23\u001b[0m             summary_method\u001b[38;5;241m=\u001b[39msummary_method,\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     25\u001b[0m         )\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _feature_vec:\n\u001b[0;32m     27\u001b[0m             feature_vec\u001b[38;5;241m.\u001b[39mextend(_feature_vec)\n",
      "File \u001b[1;32mD:\\Users\\kgunnar1\\Documents\\Kristin\\NeuroLogic\\Research\\github\\kmg\\..\\episcalp\\episcalp\\features.py:207\u001b[0m, in \u001b[0;36mheatmap_features2\u001b[1;34m(feature_map, types, ch_names, summary_method, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \n\u001b[0;32m    206\u001b[0m         #     distribution_vals.append(entropy(this_data_, uni_dist))  # kl divergence from uniform\n\u001b[1;32m--> 207\u001b[0m \n\u001b[0;32m    208\u001b[0m         #     big_feature_vec.extend(distribution_vals)\n\u001b[0;32m    209\u001b[0m \n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\Users\\kgunnar1\\Anaconda3\\envs\\episcalp\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1033\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nanmean_dispatcher)\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnanmean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    Compute the arithmetic mean along the specified axis, ignoring NaNs.\u001b[39;00m\n\u001b[0;32m    958\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1033\u001b[0m     arr, mask \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(arr, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[0;32m   1036\u001b[0m                        where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32mD:\\Users\\kgunnar1\\Anaconda3\\envs\\episcalp\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:107\u001b[0m, in \u001b[0;36m_replace_nan\u001b[1;34m(a, val)\u001b[0m\n\u001b[0;32m    104\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     np\u001b[38;5;241m.\u001b[39mcopyto(a, val, where\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a, mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "idx = 0\n",
    "dfs = []\n",
    "for i in range(1, len(metric_names) + 1):\n",
    "    for names in itertools.combinations(metric_names, i):\n",
    "        print(f\"Using metrics: {names}\")\n",
    "        # create feature matrix\n",
    "        features = []\n",
    "        for idx in range(len(fragility_dataset[\"subject\"])):\n",
    "            feature_vec = []\n",
    "            for name in names:\n",
    "                dataset = metric_mapping[name].copy()\n",
    "                # extract data and form feature vector\n",
    "                data = dataset[\"data\"][idx]\n",
    "                ch_names = dataset[\"ch_names\"][idx]\n",
    "                for summary_method, feature_types in feature_type_dict.items():\n",
    "                    if feature_types:\n",
    "                        _feature_vec = heatmap_features2(\n",
    "                            data,\n",
    "                            ch_names=ch_names,\n",
    "                            types=feature_types,\n",
    "                            summary_method=summary_method,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                        if _feature_vec:\n",
    "                            feature_vec.extend(_feature_vec)\n",
    "            features.append(feature_vec)\n",
    "\n",
    "        features = np.array(features)\n",
    "\n",
    "        dataset = fragility_dataset\n",
    "\n",
    "        # get the y-labels\n",
    "        subjects = np.array(dataset[\"subject\"])\n",
    "        roots = dataset[\"roots\"]\n",
    "\n",
    "        # get the experimental conditions\n",
    "        exp_conditions = []\n",
    "        for subject, root in zip(subjects, roots):\n",
    "            subj_df = _get_exp_condition(subject, root)\n",
    "            exp_condition = subj_df[\"exp_condition\"].values[0]\n",
    "            exp_conditions.append(exp_condition)\n",
    "\n",
    "        # encode y label\n",
    "        y = y_enc.fit_transform(exp_conditions)\n",
    "        y_classes = y_enc.classes_\n",
    "        y = np.array(convert_experimental_cond_to_y(np.array(exp_conditions)))\n",
    "        X = features\n",
    "        # Further subset the subjects if desired\n",
    "        X, y, keep_subjects = exclude_subjects(\n",
    "            X,\n",
    "            y,\n",
    "            subjects,\n",
    "            bids_roots,\n",
    "            categorical_exclusion_criteria,\n",
    "            continuous_exclusion_criteria,\n",
    "        )\n",
    "\n",
    "        max_features = X.shape[1]\n",
    "        if not rf_model_params.get(\"max_features\"):\n",
    "            rf_model_params[\"max_features\"] = max_features\n",
    "\n",
    "        if clf_name == \"rf\":\n",
    "            clf = RandomForestClassifier(**rf_model_params)\n",
    "        elif clf_name == \"sporf\":\n",
    "            # only used if you installed cysporf\n",
    "            clf = ObliqueForestClassifier(**rf_model_params)\n",
    "        elif clf_name == \"lr\":\n",
    "            clf = LogisticRegression(**lr_model_params)\n",
    "\n",
    "        # for multiclass\n",
    "        # clf = OneVsRestClassifier(clf)\n",
    "\n",
    "        steps = []\n",
    "        if clf_name == \"lr\":\n",
    "            steps.append(StandardScaler())\n",
    "        steps.append(clf)\n",
    "\n",
    "        clf = make_pipeline(*steps)\n",
    "\n",
    "        # fit on entire dataset\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        scoring_funcs = {\n",
    "            \"balanced_accuracy\": balanced_accuracy_score,\n",
    "            \"cohen_kappa_score\": cohen_kappa_score,\n",
    "            \"roc_auc\": roc_auc_score,  #  \"roc_auc\",  # roc_auc_score,\n",
    "            \"f1\": f1_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"specificity\": recall_score,\n",
    "            \"precision\": precision_score,\n",
    "            \"jaccard\": jaccard_score,\n",
    "            \"average_precision\": average_precision_score,\n",
    "            \"neg_brier_score\": brier_score_loss,\n",
    "            \"cohen_kappa_score\": cohen_kappa_score,\n",
    "            #     'specificity': '',\n",
    "        }\n",
    "\n",
    "        # evaluate the model performance\n",
    "        train_scores = dict()\n",
    "        for score_name, score_func in scoring_funcs.items():\n",
    "            y_pred_proba = clf.predict_proba(X)\n",
    "            if score_name == \"specificity\":\n",
    "                score_func = make_scorer(score_func, pos_label=0)\n",
    "            else:\n",
    "                score_func = make_scorer(score_func)\n",
    "            score = score_func(clf, X, y)\n",
    "\n",
    "            train_scores[score_name] = score\n",
    "\n",
    "        for idx in np.unique(y):\n",
    "            print(f\"Class {idx} has \", len(np.argwhere(y == idx)))\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        cv_scores = {}\n",
    "        for cv_name, cv in cvs.items():\n",
    "            # run cross-validation\n",
    "            scores = cross_validate(\n",
    "                clf,\n",
    "                X,\n",
    "                y,\n",
    "                groups=keep_subjects,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                return_estimator=True,\n",
    "                return_train_score=False,\n",
    "                n_jobs=-1,\n",
    "                error_score=\"raise\",\n",
    "            )\n",
    "\n",
    "            # get the estimators\n",
    "            estimators = scores.pop(\"estimator\")\n",
    "            cv_scores[cv_name] = scores\n",
    "\n",
    "        result_df = pd.DataFrame()\n",
    "        idx = 0\n",
    "\n",
    "        result_df[\"exp\"] = \"\"\n",
    "        result_df.at[1, \"exp\"] = idx\n",
    "        result_df[\"heatmaps\"] = \"\"\n",
    "        result_df.at[1, \"heatmaps\"] = str(names)\n",
    "        result_df[\"data_shape\"] = str(X.shape)\n",
    "        result_df[\"n_splits\"] = n_splits\n",
    "        result_df[\"n_classes\"] = len(y_enc.classes_)\n",
    "        result_df[\"clf\"] = clf_name\n",
    "\n",
    "        for name, score in train_scores.items():\n",
    "            result_df[f\"train_{name}\"] = score\n",
    "\n",
    "        for name, scores in cv_scores.items():\n",
    "            for metric, score in scores.items():\n",
    "                if not metric.startswith(\"test_\"):\n",
    "                    continue\n",
    "\n",
    "                result_df[f\"{name}_{metric}\"] = \"\"\n",
    "                result_df.at[1, f\"{name}_{metric}\"] = score\n",
    "                result_df[f\"{name}_{metric}_avg\"] = np.mean(score)\n",
    "                result_df[f\"{name}_{metric}_std\"] = np.std(score)\n",
    "        dfs.append(result_df)\n",
    "\n",
    "result_df = pd.concat(dfs)\n",
    "display(result_df)\n",
    "result_df.to_csv(fname, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094655b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(y)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b110ef0",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea113827",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name = \"lr\"\n",
    "# exp_name = \"singularvalue-first2\"\n",
    "fname = deriv_dir / \"normaleeg\" / clf_name / f\"{exp_name}_features.csv\"\n",
    "print(fname)\n",
    "\n",
    "result_df = pd.read_csv(fname, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e926ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.shape)\n",
    "# melt the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10facf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "y = \"train_roc_auc\"\n",
    "x = np.arange(len(result_df))\n",
    "ax.plot(x, result_df[y], \"*\")\n",
    "ax.set(title=f\"{clf_name} \", xlabel=\"Exp indices\", ylabel=y)\n",
    "ax.axhline([0.5], ls=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ceb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "y = \"stratifiedshuffle_test_roc_auc_avg\"\n",
    "x = np.arange(len(result_df))\n",
    "ax.plot(x, result_df[y], \"*\")\n",
    "ax.set(title=f\"{clf_name} \", xlabel=\"Exp indices\", ylabel=y)\n",
    "ax.axhline([0.5], ls=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fc699",
   "metadata": {},
   "source": [
    "# Get feature Names of top Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"train_roc_auc\"\n",
    "ordered_index = np.argsort(result_df[y])[::-1]\n",
    "keep = ordered_index[:10]\n",
    "names = result_df[\"heatmaps\"][keep].values\n",
    "\n",
    "print(result_df[y][ordered_index[:10]])\n",
    "print(f\"Best names: {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"stratifiedshuffle_test_roc_auc_avg\"\n",
    "ordered_index = np.argsort(result_df[y])[::-1]\n",
    "names = result_df[\"heatmaps\"][ordered_index[:10]].values\n",
    "\n",
    "print(result_df[y][ordered_index[:10]])\n",
    "pprint(f\"Best names: {names}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "episcalp",
   "language": "python",
   "name": "episcalp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
